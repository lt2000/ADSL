# 22.2.24

## 常见的数据分析应用

* TPC-DS
* CloudSort
* Big Data benchmark

# 22.2.25

## TPC

* 事务处理性能委员会，制定计算机事务处理能力测试标准并监督其执行

## TPC-DC

* 搞清楚几张表

# 22.2.27

## Pareto-optimal   

* 是指[资源分配](https://baike.baidu.com/item/资源分配/2944359)的一种理想状态，假定固有的一群人和可分配的资源，从一种分配状态到另一种状态的变化中，在没有使任何人境况变坏的前提下，使得至少一个人变得更好，这就是帕累托改进或帕累托最优化，**帕累托最优状态就是不可能再有更多的[帕累托改进](https://baike.baidu.com/item/帕累托改进/9595186)的余地**

## MapReduce

* 分布式计算模型

# 22.2.28

## PPT

* 第一页：Serverless
  * 存算分离
  * 高弹性、细粒度收费
* 第二页：Data analytics
  * 特征：多Stage、stage间有shuffle操作
  * 通过remote storage exchange intermediate data，导致较高的I/O时延
  * ![image-20220228144500774](C:\Users\asus\Desktop\image-20220228144500774.png)
  * 应用：Mapreduce sort、TPC-DS query

* 第三页：Idea
  * 明确stage间和stage内的函数对数据共享需求的差别

## PyWren Occupy the Cloud: Distributed Computing for the 99%  

* S3的网络带宽不是瓶颈，IOPS是瓶颈
* IOPS、带宽、latency对应用影响的权重

# 22.3.1

## Serverless数据分析类应用的I/O时延问题研究

* 这里的I/O时延是端到端的时延，包括传输时延和读写时延，即网络IO和磁盘IO

### S3、ElastiCache latency 网络时延

  * 100 measurements are carried out using the setupshown in Fig. 3. X-Ray is used for timing the measurements
    that first write random string data to the same key (step 1 )and then immediately read the data back (step 2 ).  

    <img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20220301155946289.png" alt="image-20220301155946289" style="zoom:67%;" />

  * ![image-20220301160021118](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20220301160021118.png)

### S3、ElastiCache network /网络带宽

* the gap between network bandwidth and storage I/O bandwidth is narrowing  

  <img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20220301160844877.png" alt="image-20220301160844877" style="zoom:67%;" />

### S3、ElastiCache request throughput (IOPS)

* ![image-20220301163938617](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20220301163938617.png)



## CaaS vs FaaS

* K8s和Serverless的区别是什么？
  * CaaS
    * The first one enables users to run application components in highly customizable containers but thus makes configuration harder  
  * FaaS
    * FaaS tries to solve the configuration issue by providing fixed execution environments where small, stateless functions could be run  

# 22.3.2

* AWS serverless 完整的技术栈：https://medium.com/serverless-transformation/what-a-typical-100-serverless-architecture-looks-like-in-aws-40f252cd0ecb

# 22.3.3

## pywren

* 是不是已经做了这样的存储优化？？？？？？

# 22.3.8

## Spark怎么处理中间数据？

## AWS lambda怎么处理中间数据？

## 几篇文章

* pywren
* numpywren
* wukong
* pocket(两篇)

## Understanding Ephemeral Storage for Serverless Analytics  

* In traditional analytics frameworks (e.g., Spark, Hadoop), ==tasks buffer intermediate data in local storage and exchange data between tasks directly== over the network  

* serverless computing frameworks achieve high elasticity and scalability by requiring tasks to be stateless [15].In other words, ==a task’s local file system and child processes are limited to the lifetime of the task itself==

* ==since serverless platforms do not expose control over task scheduling and placement, direct communication between tasks is difficult==  

* the natural approach for inter-task communication is to store intermediate data in a common, remote storage service  

* **Three different serverless analytics applications** 

  * Parallel software build ：
    * Each lambda fetches its dependencies from ephemeral storage
    * computes (i.e., compiles, archives or links depending on the stage)
    * and writes an output file  
  * MapReduce Sort ：
    * Map lambdas fetch input files from long-termstorage (S3) and write intermediate files to ephemeralstorage. 
    * Reduce lambdas merge and sort intermediate data read from ephemeral storage and write output files to S3  
    * Sorting is I/O-intensive  
    * ==Each intermediate file is written and read only once== and its size is directly proportional to the dataset size and inversely related to the number of workers  
  * Video analytics ：
    * **decode stage**：first stage lambdas read a batch of encoded video frames from ephemeral storage and write back decoded video frames 
    *  **MXNET classification stage**：Each lambda then launches a second stage lambda which reads a set of decoded frames from ephemeral storage, computes a MXNET deep learning classification algorithm and outputs a classification result  

* **Three different storage systems for ephemeral data sharing**

  * <img src="C:\Users\taoli\AppData\Roaming\Typora\typora-user-images\image-20220308163825575.png" alt="image-20220308163825575" style="zoom: 67%;" />
  * <img src="C:\Users\taoli\AppData\Roaming\Typora\typora-user-images\image-20220308163908002.png" alt="image-20220308163908002" style="zoom:67%;" />

  * S3
    * S3 has significant overhead, particularly for small requests  
  * Elasticache Redis  
    * Redis latency is ∼240 ms, two orders of magnitude lower than S3 latency  
  * Crail-ReFlex: 
    * Flash offers a medium ground between disk and DRAM for both performance and cost  

* latency sensitivity, parallelism, and I/O intensity  
  * Latency-sensitive jobs: 
    * We find that jobs in which lambdas mostly issue fine-grain I/O operations are latency-sensitive ;
    * gg shows some sensitivity to storage latency since the majority of files accessed are under 100 KB  
    * The job benefits from the lower latency of Redis storage compared to S3 with up to 100 concurrent lambdas  
  * Jobs with limited parallelism  
  * Throughput-intensive jobs  
* ==Desired ephemeral storage properties==
  * high elasticity ：To meet the I/O demands of serverless applications, which can consist of thousands of lambdas in one execution stage and
    only a few lambdas in another  
  * high IOPS and high throughput ：Since the granularity of data access varies widely (Figure 2), storing both small and large objects should be cost and performance efficient  
  * auto-scale resources  ：To relieve users from the difficulty of managing storage clusters, the storage service  

# 22.3.9

## unloaded latency & loaded latency

* Unloaded latency **measures the round-trip time of a request when there is no other traffic present on a user's network**, 
* while loaded latency measures the round-trip time when data-heavy applications are being used on the network. For example, let's say you're playing a game online on your computer

# 22.3.14

## 边缘计算 vs 云计算

## AWS limitation

* The default limit for concurrent executions is a maximum of 1000 lambdas, 512MB of temporary storage and 900 seconds of timeout

## 为什么S3对于小文件的读写不支持高吞吐率

* In fact, all major public cloud providers impose a global transaction limit on shared object stores [37, 7, 20]. This should come as no surprise, as starting with the Google File System [21], the majority of large scale storage systems have been optimized for reading and writing large chunks of data, rather than for high-throughput fine-grained operations ==Locus==
* While S3 does provide abundant I/O bandwidth to Lambda for this case, it is not designed to sustain high request rate for small objects. Also as S3 is a multi-tenant service, there is an imposed limit on request throughput per S3 bucket for the benefit of overall availability ==Pywren==  

## 典型的数据分析应用的I/O size到底会不会导致S3的I/O吞吐率成为瓶颈？

* 首先要得到数据分析应用的I/O size的分布函数图(CDF)
* 然后获得redis/S3的聚合带宽和I/O size的关系图
* 判断数据分析应用的I/O size有多大的规模落在S3 I/O throughput的瓶颈区 



# 22.3.23

## S3的适用场景

* S3 is usually meant for storing small number of large sized objects which are infrequently accessed and the latency is not that of significant importance. For instance,
  1. You store log files once every few minutes or hours
  2. Log file size are on the order of several hundred MB's or couple of GB's
  3. Read them few hundreds times
  4. Latency of even around couple of seconds is acceptable

# 22.3.31

## schedule overhead

* need to rapidly scale and schedule
  tasks at high throughput, while minimizing data movement
  across tasks.  
* decentralized scheduling enables scheduling to be
  distributed across Lambda executors that can schedule tasks
  in parallel, and brings multiple benefits, including enhanced
  data locality, reduced network I/Os, automatic resource elasticity, and improved cost effectiveness  
* a logically centralized scheduler for managing task assignments and resource allocation  
* the serverless
  invocation model imposes non-trivial scheduling overhead  
* This is because, while serverless computing promises
  to deliver elastic auto-scaling feature in response to bursts
  of concurrent workloads, serverless function invocations
  incur non-negligible overhead, thus creating a scheduling
  bottleneck with slow scaling out  
* 传统的中心调度方式：master schedule（trigger、invoke、assign、track）
  * One significant overhead results from the master-side
    workflow schedule pattern, with which the functions are triggered
    in the master node and assigned to worker nodes for execution  
  * These serverless workflow systems usually provide a centralized workflow engine on the master node to manage the workflow execution state
    and assign function tasks to the worker nodes  
  * On the one hand, the central workflow engine is responsible
    for dynamically managing and scheduling all functions. The function execution states are frequently transferred from the master
    node to the worker nodes, introducing heavy scheduling overhead  
  * In this case, the master node is needed in
    a workflow engine, which maintains the functions and workflows
    states. Master node collects the execution states of functions from  the worker nodes and determines whether functions in the workflow meet their trigger conditions  

## data movement overhead

* serverless workflows must rely on external cloud store for intermediate data
  storage and exchange, which creates excessive data movement overhead  
* While
  the stateless design seems to be a good fit for serverless
  platforms, it does not preserve data locality, which results in
  excessive data movement  



# 22.4.4

## 生成排序数据的工具

* http://www.ordinal.com/gensort.html

## 毕业论文提纲

* 介绍serverless优点特性
* 介绍data analytics application以及serverless对这类应用的吸引力
* 介绍将这类应用部署在serverless上的挑战
  * 放上用Lithops跑的实验数据
* related work
  * distributed computing：pywren、IBM-pywren、Lithops
  * exchange operator：starling、lambada、boxer
  * decentralized schedule：wukong、faasflow
  * replacing disk-based object storage with memory-based storage , or combining different storage media：pocket、locus
  * application aware data passing：Sonic
* my idea
  * lithops + multilevel exchange operator
* future work
  * lithops + multilevel exchange operator + decentralized schedule

## 下一步计划安排

### lithops

1. Pywren -> IBM-Pywren -> Lithops
   *  搞清楚每个系统的story和advantage
2. 选择Lithops的实验平台
   * 阿里云
   * AWS
3. 运行lithops
   * wordcount、terasort
   * realworld application

### multistage shuffle

# 22.4.5

## Pywren

* Map-reduce style serverless framework for highly parallel analytics workloads

### 实现的分布式计算模型

#### Map

* highly parallel analytics workloads

#### Map + monolithic Reduce  

* this pattern covers a number of classical machine learning workloads which consist of a featurization (or ETL) stage that converts large input data into features and then a learning stage where the model is built using SVMs or linear classifiers  

#### MapReduce

* word count
* terasort

#### Parameter Servers  



# 22.4.6

* 注册AWS账户

* 配置lithops+aws function+s3

* test examples

* 明天

  * 

## BOKI

* 

## Writing

### 开始写作

* design没有讲清楚
* 一直在做实验，前期与老师讨论实验规划
* 论文格式
* introduction很重要

# 22.4.8

* aws_lambda、aws_s3 benchmark

  * lambda：并发度

  * S3：时延、带宽

  * Access Key ID:

    AKIA3MTLAMVJ537BMP7H

    Secret Access Key:

    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

# 22.4.9

* PPT

  * 讲清楚pywren、lithops
    * mapreduce style
  * 测试
    * serverless函数的并发性测试
    * 对象存储的读写带宽测试
    * wordcount的breakdown
      * s3  I/O
      * computing
      * shuffle
  * wordcount + multistage shuffle

* 毕设

  * 第一章：Serverless

    * serverless的架构：FaaS + BaaS、Workflow
    * serverless的特性：资源高弹性、细粒度计费等

  * 第二章：Serverless state management

    * Function is Stateless
    * Serverless data analytics application

  * 第三章：Related work

    * 存储优化：locus、pocket、sonic
    * 调度优化：wukong、faasflow

  * 第四章：Design

    * Decentralized schedule
    * multistage shuffle

  * 第五章：Implementation

    * 原型系统：Lithops
    * 实现两种应用：wordcount、terasort
    * 实现multistage shuffle

  * 第六章：Evaluation

    * 实验环境：
  
      * 本地：node8
    
      * 云：阿里云FC、阿里云OSS
    * 实验设计
      * wordcount/terasort breakdown：S3 I/O、Computing、Shuffle I/O
      * wordcount/terasort + multistage breakdown
    
    
  
* LIthops mapreduce
  
  * 如何实现的shuffle？？？
  
    
